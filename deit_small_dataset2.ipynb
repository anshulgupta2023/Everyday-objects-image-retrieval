{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip images.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cdPD4qb_wyii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoFeatureExtractor, ViTForImageClassification\n",
        "from PIL import Image\n",
        "\n",
        "# Load pretrained model + feature extractor\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/deit-small-patch16-224\")\n",
        "model = ViTForImageClassification.from_pretrained(\"facebook/deit-small-patch16-224\")"
      ],
      "metadata": {
        "id": "kgZQJWdMxQML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "finetune"
      ],
      "metadata": {
        "id": "RdiDCOi1Jd_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "azUKQIUZ6szp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    ViTModel,\n",
        "    AutoFeatureExtractor  # Using AutoFeatureExtractor like in your example\n",
        ")\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "# Suppress harmless warnings from pandas\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "id": "w1_yKJBNqb5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. CONFIGURATION ---\n",
        "\n",
        "# âœ… Update paths as needed\n",
        "SINGLE_CSV_PATH = \"/content/dataset_clean_split.csv\"  # Path to the cleaned dataset\n",
        "IMAGE_BASE_DIR = \"/content/\"  # Base directory for your image paths\n",
        "\n",
        "# --- Attribute tasks to predict ---\n",
        "# You can select a subset or use all of them, depending on your model setup\n",
        "ATTRIBUTE_TASKS = [\n",
        "    'color',\n",
        "    'material',\n",
        "    'condition',\n",
        "    'size',\n",
        "    'pattern',\n",
        "    'features',\n",
        "    'shape',\n",
        "    'style'\n",
        "]\n",
        "# -----------------------------------\n",
        "\n",
        "# Model Configuration\n",
        "# We can use the 'patch16' or 'distilled' version. 'distilled' is often a bit better.\n",
        "MODEL_CHECKPOINT = \"facebook/deit-small-patch16-224\"\n",
        "MODEL_NAME = \"deit-small-finetuned\"\n",
        "\n",
        "# Training Configuration\n",
        "LEARNING_RATE = 2e-5 # <--- CRITICAL: Low LR for fine-tuning\n",
        "BATCH_SIZE = 16 # Adjust based on your Colab GPU (T4 should handle 16)\n",
        "NUM_EPOCHS = 10 # Start with 10, can increase later"
      ],
      "metadata": {
        "id": "W9WzdD3x523a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_attributes(attr_str):\n",
        "    \"\"\"\n",
        "    Parses a string like 'color:white;material:plastic'\n",
        "    into a dictionary {'color': 'white', 'material': 'plastic'}\n",
        "    \"\"\"\n",
        "    if pd.isna(attr_str):\n",
        "        return {}\n",
        "    attributes = {}\n",
        "    try:\n",
        "        for item in attr_str.split(';'):\n",
        "            if ':' in item:\n",
        "                key, value = item.split(':', 1)\n",
        "                attributes[key.strip()] = value.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing attribute string: {attr_str} | Error: {e}\")\n",
        "    return attributes\n",
        "\n",
        "def preprocess_dataframe(csv_path, attribute_task_list):\n",
        "    \"\"\"\n",
        "    Loads the single CSV, parses *only* the specified attributes,\n",
        "    and returns train/val dataframes and task info.\n",
        "    \"\"\"\n",
        "    print(f\"Loading data from {csv_path}...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    print(\"Parsing 'attributes' column...\")\n",
        "    attr_dicts = df['attributes'].apply(parse_attributes)\n",
        "    attr_df = pd.DataFrame(attr_dicts.tolist())\n",
        "\n",
        "    print(f\"Filtering for specified attributes: {attribute_task_list}\")\n",
        "    found_attributes = [col for col in attribute_task_list if col in attr_df.columns]\n",
        "    missing_attributes = [col for col in attribute_task_list if col not in attr_df.columns]\n",
        "\n",
        "    if missing_attributes:\n",
        "        print(f\"Warning: Could not find columns for attributes: {missing_attributes}\")\n",
        "\n",
        "    attr_df = attr_df[found_attributes]\n",
        "\n",
        "    df = pd.concat([df, attr_df], axis=1)\n",
        "    df[found_attributes] = df[found_attributes].fillna('N/A')\n",
        "\n",
        "    task_columns = ['class_label'] + found_attributes\n",
        "\n",
        "    df_train = df[df['split'] == 'train'].reset_index(drop=True)\n",
        "    df_val = df[df['split'] == 'val'].reset_index(drop=True)\n",
        "\n",
        "    print(f\"Created {len(df_train)} training samples and {len(df_val)} validation samples.\")\n",
        "    return df_train, df_val, task_columns"
      ],
      "metadata": {
        "id": "ebsqgmse6YTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_label_maps(df_train, df_val, task_cols):\n",
        "    \"\"\"Creates label-to-id and id-to-label maps for all tasks.\"\"\"\n",
        "    label2id = defaultdict(dict)\n",
        "    id2label = defaultdict(dict)\n",
        "\n",
        "    print(\"Creating label maps...\")\n",
        "    for col in task_cols:\n",
        "        unique_labels = pd.concat([df_train[col], df_val[col]]).unique()\n",
        "        unique_labels.sort()\n",
        "\n",
        "        label2id[col] = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "        id2label[col] = {idx: label for idx, label in enumerate(unique_labels)}\n",
        "        print(f\"  Task '{col}': {len(unique_labels)} classes (e.g., {unique_labels[0]})\")\n",
        "\n",
        "    return label2id, id2label"
      ],
      "metadata": {
        "id": "aBZNl_Lz6acw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskImageDataset(Dataset):\n",
        "    def __init__(self, df, feature_extractor, label2id_maps, task_cols, image_base_dir):\n",
        "        self.df = df\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.label2id_maps = label2id_maps\n",
        "        self.task_cols = task_cols\n",
        "        self.image_base_dir = image_base_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image_path = os.path.join(self.image_base_dir, row['image_path'])\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            pixel_values = self.feature_extractor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            # On error, load the first image as a placeholder\n",
        "            row0 = self.df.iloc[0]\n",
        "            image0_path = os.path.join(self.image_base_dir, row0['image_path'])\n",
        "            image = Image.open(image0_path).convert(\"RGB\")\n",
        "            pixel_values = self.feature_extractor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze()\n",
        "\n",
        "        labels = []\n",
        "        for col in self.task_cols:\n",
        "            label_str = row[col]\n",
        "            label_id = self.label2id_maps[col][label_str]\n",
        "            labels.append(label_id)\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "5yXGtViT6dTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 7 (Corrected) ---\n",
        "from transformers import AutoModel # <-- Import AutoModel\n",
        "\n",
        "class MultiTaskDeiT(nn.Module):\n",
        "    def __init__(self, model_name, num_labels_per_task):\n",
        "        super().__init__()\n",
        "        self.num_tasks = len(num_labels_per_task)\n",
        "\n",
        "        # 1. Load the backbone using AutoModel (UN-FROZEN by default)\n",
        "        # This will correctly load a DeiTModel class\n",
        "        self.vit = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        # 2. Create a separate classification head for each task\n",
        "        self.heads = nn.ModuleList()\n",
        "        for num_labels in num_labels_per_task:\n",
        "            head = nn.Sequential(\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(self.vit.config.hidden_size, num_labels)\n",
        "            )\n",
        "            self.heads.append(head)\n",
        "\n",
        "    def forward(self, pixel_values, labels=None):\n",
        "        outputs = self.vit(pixel_values=pixel_values)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        logits_list = [head(cls_output) for head in self.heads]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = 0\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            for i in range(self.num_tasks):\n",
        "                logits = logits_list[i]\n",
        "                task_labels = labels[:, i]\n",
        "                loss += loss_fct(logits.view(-1, self.heads[i][1].out_features), task_labels.view(-1))\n",
        "\n",
        "            loss = loss / self.num_tasks # Average loss\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"logits\": tuple(logits_list)\n",
        "        }"
      ],
      "metadata": {
        "id": "qg6OI8Xs6d3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need global vars for compute_metrics to see the task columns\n",
        "g_task_columns = []\n",
        "# Load the accuracy metric\n",
        "g_accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits_tuple, labels = eval_pred\n",
        "    metrics = {}\n",
        "\n",
        "    for i, task_name in enumerate(g_task_columns):\n",
        "        task_logits = logits_tuple[i]\n",
        "        task_labels = labels[:, i]\n",
        "\n",
        "        preds = np.argmax(task_logits, axis=1)\n",
        "\n",
        "        # Use the evaluate library to compute accuracy\n",
        "        acc = g_accuracy_metric.compute(predictions=preds, references=task_labels)\n",
        "        metrics[f\"{task_name}_accuracy\"] = acc[\"accuracy\"] # extract the value\n",
        "\n",
        "    metrics[\"average_accuracy\"] = np.mean(list(metrics.values()))\n",
        "    return metrics\n",
        "\n",
        "# The Trainer's default collator is fine, but this is good practice\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.stack([x['labels'] for x in batch])\n",
        "    }"
      ],
      "metadata": {
        "id": "cGzCh91M6gGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Pre-process the single CSV\n",
        "df_train, df_val, task_columns = preprocess_dataframe(SINGLE_CSV_PATH, ATTRIBUTE_TASKS)\n",
        "g_task_columns = task_columns # Set global var for compute_metrics\n",
        "\n",
        "# 2. Create label maps\n",
        "label2id, id2label = create_label_maps(df_train, df_val, task_columns)\n",
        "\n",
        "# Get the number of labels for each task\n",
        "num_labels_per_task = [len(label2id[col]) for col in task_columns]\n",
        "print(f\"\\nFinal tasks and label counts: {list(zip(task_columns, num_labels_per_task))}\")\n",
        "\n",
        "# 3. Initialize feature extractor\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# 4. Create datasets\n",
        "train_dataset = MultiTaskImageDataset(df_train, feature_extractor, label2id, task_columns, IMAGE_BASE_DIR)\n",
        "val_dataset = MultiTaskImageDataset(df_val, feature_extractor, label2id, task_columns, IMAGE_BASE_DIR)\n",
        "print(f\"\\nLoaded {len(train_dataset)} training examples and {len(val_dataset)} validation examples.\")"
      ],
      "metadata": {
        "id": "R4pLkHhr64Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 10 (Corrected) ---\n",
        "\n",
        "# 1. Initialize model\n",
        "model = MultiTaskDeiT(MODEL_CHECKPOINT, num_labels_per_task)\n",
        "\n",
        "# 2. Set up Training Arguments for FINE-TUNING\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=MODEL_NAME,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE, # <-- LOW LR\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    logging_steps=20,\n",
        "    eval_strategy=\"epoch\",  # <-- RENAMED\n",
        "    save_strategy=\"epoch\",  # <-- RENAMED\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"average_accuracy\",\n",
        "    greater_is_better=True,\n",
        "    remove_unused_columns=False, # We need 'labels', so set this to False\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# 3. Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "Ndxmek5_69FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_path = \"/content/dataset_clean_split.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Replace backslashes with forward slashes in image paths\n",
        "df['image_path'] = df['image_path'].str.replace('\\\\', '/', regex=False)\n",
        "\n",
        "# Save back\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(\"âœ… Fixed image paths in:\", csv_path)"
      ],
      "metadata": {
        "id": "2aHcp19sZuwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting fine-tuning...\")\n",
        "print(f\"Backbone: {MODEL_CHECKPOINT} (UNFROZEN)\")\n",
        "print(f\"Tasks: {g_task_columns}\")\n",
        "print(f\"LR: {LEARNING_RATE}, Epochs: {NUM_EPOCHS}, Batch Size: {BATCH_SIZE}\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"Fine-tuning complete.\")"
      ],
      "metadata": {
        "id": "VXpuM5S47wgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final best model\n",
        "trainer.save_model(f\"{MODEL_NAME}/final_model_2\")\n",
        "print(f\"Best model saved to {MODEL_NAME}/final_model_2\")"
      ],
      "metadata": {
        "id": "dyi_S9uR7_7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "retrieval system - search index based"
      ],
      "metadata": {
        "id": "1CDki8VDEteI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import spacy\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "from transformers import AutoFeatureExtractor, AutoModel\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "class Config:\n",
        "    CSV_PATH = \"/content/dataset_clean_split.csv\"\n",
        "    ATTRIBUTE_COLS = ['class_label',\n",
        "    'color',\n",
        "    'material',\n",
        "    'condition',\n",
        "    'size',\n",
        "    'pattern',\n",
        "    'features',\n",
        "    'shape',\n",
        "    'style'\n",
        "]\n",
        "    MODEL_NAME = \"facebook/deit-small-patch16-224\" # Must match the model you trained\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    RANDOM_SEED = 42\n",
        "\n",
        "np.random.seed(Config.RANDOM_SEED)\n",
        "torch.manual_seed(Config.RANDOM_SEED)\n",
        "\n",
        "print(f\"Using device: {Config.DEVICE}\")\n",
        "print(f\"Using base model: {Config.MODEL_NAME}\")\n",
        "\n",
        "# --- 2. MODEL CLASS DEFINITION (MODIFIED) ---\n",
        "# This class is now designed to match the keys in your .safetensors file\n",
        "# e.g., \"vit.embeddings...\" and \"heads.0.1.weight\"\n",
        "class MultiAttributeModel(nn.Module):\n",
        "    def __init__(self, model_name, label_to_id_maps):\n",
        "        super().__init__()\n",
        "        self.label_maps = label_to_id_maps\n",
        "        self.attributes = list(label_to_id_maps.keys())\n",
        "\n",
        "        # 1. Backbone key is 'vit' (to match your file)\n",
        "        self.vit = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        self.hidden_dim = self.vit.config.hidden_size\n",
        "\n",
        "        # 2. Heads are a ModuleList (to match 'heads.0', 'heads.1', etc.)\n",
        "        #    and each head is a Sequential (to match 'heads.0.1.weight')\n",
        "        self.heads = nn.ModuleList()\n",
        "        for attr in self.attributes:\n",
        "            num_classes = len(self.label_maps.get(attr, {}))\n",
        "            if num_classes == 0:\n",
        "                print(f\"Warning: No classes found for attribute '{attr}'. Head will be a placeholder.\")\n",
        "                # Add a dummy module to keep indices aligned\n",
        "                self.heads.append(nn.Identity())\n",
        "            else:\n",
        "                # This nn.Sequential block creates keys like 'heads.0.1.weight'\n",
        "                # (where '0' is the index in the ModuleList and '1' is the index in Sequential)\n",
        "                self.heads.append(\n",
        "                    nn.Sequential(\n",
        "                        nn.Identity(), # Placeholder for index 0\n",
        "                        nn.Linear(self.hidden_dim, num_classes) # Your weights at index 1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    def forward(self, pixel_values, labels=None):\n",
        "        # 1. Use self.vit\n",
        "        backbone_outputs = self.vit(pixel_values)\n",
        "        pooled_output = backbone_outputs.last_hidden_state[:, 0]\n",
        "\n",
        "        # 2. Iterate and use the ModuleList\n",
        "        logits_dict = {}\n",
        "        for i, attr in enumerate(self.attributes):\n",
        "            # Check if the head is a real layer (not the placeholder)\n",
        "            if isinstance(self.heads[i], nn.Sequential):\n",
        "                logits_dict[attr] = self.heads[i](pooled_output)\n",
        "\n",
        "        outputs = {\"logits\": logits_dict}\n",
        "        return outputs\n",
        "\n",
        "# --- 3. DATASET CLASS DEFINITION ---\n",
        "# This is needed to load the images to build the index\n",
        "class ProductAttributeDataset(Dataset):\n",
        "    def __init__(self, df, feature_extractor, label_to_id_maps):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.label_to_id = label_to_id_maps\n",
        "        self.attributes = list(label_to_id_maps.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image_path = row['image_path']\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            processed_image = self.feature_extractor(\n",
        "                images=image,\n",
        "                return_tensors=\"pt\"\n",
        "            )['pixel_values'].squeeze(0)\n",
        "        except Exception as e:\n",
        "            # Suppress warnings during demo\n",
        "            processed_image = torch.zeros((3, 224, 224))\n",
        "\n",
        "        # We only need the image and path for indexing\n",
        "        return {\n",
        "            \"pixel_values\": processed_image,\n",
        "            \"image_path\": image_path\n",
        "        }\n",
        "\n",
        "print(\"Setup Complete: New model architecture is defined.\")"
      ],
      "metadata": {
        "id": "YsTq0tiUEbJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Function to parse the 'attributes' column ---\n",
        "def parse_attributes_string(attr_string):\n",
        "    if pd.isna(attr_string):\n",
        "        return {}\n",
        "    attributes = {}\n",
        "    try:\n",
        "        pairs = attr_string.split(';')\n",
        "        for pair in pairs:\n",
        "            if ':' in pair:\n",
        "                key, value = pair.split(':', 1)\n",
        "                if key and value:\n",
        "                    attributes[key.strip()] = value.strip()\n",
        "    except Exception as e:\n",
        "        pass # Suppress warnings during demo\n",
        "    return attributes\n",
        "\n",
        "# --- 2. Load and Process the DataFrame ---\n",
        "print(\"Loading and processing newlabels.csv...\")\n",
        "df = pd.read_csv(Config.CSV_PATH)\n",
        "parsed_attrs = df['attributes'].apply(parse_attributes_string)\n",
        "df_attrs = pd.json_normalize(parsed_attrs)\n",
        "cols_to_drop = [col for col in df_attrs.columns if col in df.columns]\n",
        "if cols_to_drop:\n",
        "    df_attrs = df_attrs.drop(columns=cols_to_drop)\n",
        "df = df.join(df_attrs)\n",
        "for col in Config.ATTRIBUTE_COLS:\n",
        "    if col not in df.columns:\n",
        "        df[col] = np.nan\n",
        "\n",
        "# --- THIS IS THE FIX ---\n",
        "# Replicate the training script's logic\n",
        "# Replace all empty (NaN) values with the string \"N/A\"\n",
        "# This makes \"N/A\" a new, learnable class\n",
        "print(\"Replicating training logic: filling NaN with 'N/A'...\")\n",
        "\n",
        "# Get the list of columns *except* class_label\n",
        "# (Based on your log, 'class_label' did not have an 'N/A' class)\n",
        "attr_cols_to_fill = [col for col in Config.ATTRIBUTE_COLS if col != 'class_label']\n",
        "df[attr_cols_to_fill] = df[attr_cols_to_fill].fillna('N/A')\n",
        "# --- END FIX ---\n",
        "\n",
        "\n",
        "# --- 3. Dynamically create label mappings ---\n",
        "# This code now works, because .dropna() will do nothing\n",
        "# for the attribute columns (since they have no NaNs)\n",
        "# and .unique() will find 29 colors, 7 materials, etc.\n",
        "label_to_id = {}\n",
        "id_to_label = {}\n",
        "print(\"Building label mappings...\")\n",
        "for col in Config.ATTRIBUTE_COLS:\n",
        "    unique_values = df[col].dropna().unique() # We can keep .dropna() for class_label\n",
        "    unique_values.sort()\n",
        "    label_to_id[col] = {label: idx for idx, label in enumerate(unique_values)}\n",
        "    id_to_label[col] = {idx: label for idx, label in enumerate(unique_values)}\n",
        "    print(f\"  Found {len(unique_values)} classes for {col}\") # Debug print\n",
        "\n",
        "# --- 4. Load Feature Extractor and Dataset ---\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(Config.MODEL_NAME)\n",
        "full_dataset = ProductAttributeDataset(df, feature_extractor, label_to_id)\n",
        "print(f\"\\nData ready. Full dataset contains {len(full_dataset)} images to index.\")"
      ],
      "metadata": {
        "id": "THkuJfV6F7OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from safetensors.torch import load_file\n",
        "import os # Import 'os' to check if files exist\n",
        "\n",
        "# --- 1. Load the NLP model for keyword extraction ---\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy NLP model loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading spaCy model: {e}\")\n",
        "\n",
        "# --- 2. Load your fine-tuned classifier model ---\n",
        "print(\"Loading fine-tuned classifier from .safetensors...\")\n",
        "model = MultiAttributeModel(Config.MODEL_NAME, label_to_id)\n",
        "\n",
        "# --- THIS IS THE CRITICAL LINE ---\n",
        "# Make sure your file path is correct here\n",
        "model_path = \"/content/deit-small-finetuned/final_model_2/model.safetensors\"\n",
        "# -----------------------------------\n",
        "\n",
        "model_loaded = False # Flag to track if loading was successful\n",
        "\n",
        "# --- Check if the file exists BEFORE trying to load ---\n",
        "if not model_path:\n",
        "    print(\"--- ERROR: 'model_path' variable is empty! ---\")\n",
        "elif not os.path.exists(model_path):\n",
        "    print(f\"--- ERROR ---\")\n",
        "    print(f\"File not found at: {model_path}\")\n",
        "    print(\"Please check the path and make sure 'final_model/model.safetensors' is uploaded.\")\n",
        "else:\n",
        "    # File exists, now we try to load it\n",
        "    try:\n",
        "        state_dict = load_file(model_path, device=Config.DEVICE)\n",
        "        model.load_state_dict(state_dict)\n",
        "        model.to(Config.DEVICE)\n",
        "        model.eval()\n",
        "        print(f\"Fine-tuned model loaded successfully from {model_path}\")\n",
        "        model_loaded = True # Set flag to True\n",
        "    except Exception as e:\n",
        "        print(f\"--- ERROR ---\")\n",
        "        print(f\"Could not load '{model_path}': {e}\")\n",
        "        print(\"This usually means the model architecture in Cell 2 doesn't match the saved file.\")\n",
        "\n",
        "# --- 3. Build the Search Index ---\n",
        "# --- ONLY RUN THIS IF THE MODEL LOADED SUCCESSFULLY ---\n",
        "if model_loaded:\n",
        "    full_dataset = ProductAttributeDataset(df, feature_extractor, label_to_id)\n",
        "    search_index = []\n",
        "    all_tags = set()\n",
        "\n",
        "    print(f\"Building search index from {len(full_dataset)} images...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(full_dataset))):\n",
        "            item = full_dataset[i]\n",
        "            pixel_values = item['pixel_values'].unsqueeze(0).to(Config.DEVICE)\n",
        "\n",
        "            # This line will now work\n",
        "            outputs = model(pixel_values=pixel_values)\n",
        "            logits_dict = outputs['logits']\n",
        "\n",
        "            image_data = {\"image_path\": item['image_path'], \"tags\": []}\n",
        "\n",
        "            # Convert logits to predicted tags\n",
        "            for attr in Config.ATTRIBUTE_COLS:\n",
        "                if attr in logits_dict:\n",
        "                    logits = logits_dict[attr].squeeze()\n",
        "                    pred_idx = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "                    pred_label = id_to_label[attr].get(pred_idx, \"N/A\")\n",
        "\n",
        "                    if pred_label != \"N/A\":\n",
        "                        if attr == 'class_label':\n",
        "                            tags = pred_label.split('_')\n",
        "                            image_data[\"tags\"].extend(tags)\n",
        "                            all_tags.update(tags)\n",
        "                        else:\n",
        "                            image_data[\"tags\"].append(pred_label)\n",
        "                            all_tags.add(pred_label)\n",
        "\n",
        "            search_index.append(image_data)\n",
        "\n",
        "    print(f\"--- Search index built! ---\")\n",
        "else:\n",
        "    print(\"\\n--- Model failed to load. Aborting search index build. ---\")\n",
        "    print(\"Please fix the errors above and re-run this cell.\")"
      ],
      "metadata": {
        "id": "dfvvRPQPGBkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def extract_keywords(text):\n",
        "    \"\"\"\n",
        "    Parses text with spaCy and extracts key search terms (nouns, adjectives).\n",
        "    Uses the base word (lemma) for better matching (e.g., \"bottles\" -> \"bottle\").\n",
        "    \"\"\"\n",
        "    doc = nlp(text.lower())\n",
        "    keywords = set()\n",
        "    for token in doc:\n",
        "        # We want nouns, proper nouns, and adjectives\n",
        "        if token.pos_ in (\"NOUN\", \"PROPN\", \"ADJ\"):\n",
        "            keywords.add(token.lemma_) # 'lemma_' gives the base word\n",
        "\n",
        "    # If no keywords found (e.g., \"a used shampoo\"), just split the string\n",
        "    if not keywords and text:\n",
        "         keywords = set(text.lower().split())\n",
        "\n",
        "    return keywords\n",
        "\n",
        "def retrieve_images(query_text, index, top_k=5):\n",
        "    \"\"\"\n",
        "    1. Extracts keywords from query_text using spaCy\n",
        "    2. Scores each image in the index based on keyword matches\n",
        "    3. Returns the top_k matching image paths\n",
        "    \"\"\"\n",
        "    # 1. Extract keywords from the user's query\n",
        "    query_keywords = extract_keywords(query_text)\n",
        "    if not query_keywords:\n",
        "        print(\"No keywords extracted from query.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Searching for keywords: {query_keywords}\")\n",
        "\n",
        "    # 2. Score each image in our index\n",
        "    scores = []\n",
        "    for item in index:\n",
        "        # Our \"database\" of tags predicted by your ViT\n",
        "        item_tags = set(item['tags'])\n",
        "\n",
        "        # Score is simply the number of matching keywords\n",
        "        match_score = len(query_keywords.intersection(item_tags))\n",
        "\n",
        "        if match_score > 0:\n",
        "            scores.append({\n",
        "                \"path\": item['image_path'],\n",
        "                \"score\": match_score,\n",
        "                \"matched_tags\": list(query_keywords.intersection(item_tags))\n",
        "            })\n",
        "\n",
        "    # 3. Sort by score (highest first)\n",
        "    scores.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    return scores[:top_k]\n",
        "\n",
        "# --- ðŸš€ LIVE DEMO ---\n",
        "# Try different queries here!\n",
        "# query = \"blue plastic bottle\"\n",
        "# query = \"a new leather backpack\"\n",
        "query = \"used shampoo\"\n",
        "\n",
        "retrieved_results = retrieve_images(query, search_index, top_k=5)\n",
        "\n",
        "if retrieved_results:\n",
        "    print(f\"\\n--- Top 5 results for '{query}' ---\")\n",
        "    for result in retrieved_results:\n",
        "        print(f\"Score: {result['score']} | Matched: {result['matched_tags']} | Path: {result['path']}\")\n",
        "\n",
        "    # --- Optional: Display the top image ---\n",
        "    top_image_path = retrieved_results[0]['path']\n",
        "    try:\n",
        "        img = Image.open(top_image_path)\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"Top Result (Score: {retrieved_results[0]['score']})\\n{top_image_path}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Could not display image at {top_image_path}: {e}\")\n",
        "else:\n",
        "    print(f\"No results found for '{query}'\")"
      ],
      "metadata": {
        "id": "ICTmXCPnQByi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r final_model_1.zip deit-small-finetuned/final_model_2"
      ],
      "metadata": {
        "id": "q2e4i_ucQn9A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}